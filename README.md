# üöÄ Phase 1 (NLP Keyword Matching) ‚úÖ
  
> *A resume analysis tool that uses Natural Language Processing (NLTK) to clean, lemmatize, and match resumes against Job Descriptions.*

![Smart Resume Analyzer Demo](assets/phase_1_demo.png)

# ü§ñ Phase 2 (Semantic Search & Explainability) ‚úÖ

> *A Next-Gen ATS Simulator that uses Deep Learning (SBERT Transformers) to understand the **meaning** of a resume, not just keywords.*

![Phase 2 Analysis Demo](assets/phase_2_demo.png)

# üê≥ **Current Status:** Phase 3 (Containerized Microservice) ‚úÖ
  
> *A Production-Ready ATS Simulator. Containerized with Docker for "Run Anywhere" deployment, featuring Semantic Search and AI-Powered Explainability.*

![Phase 3](assets/phase_3_demo.png)

## üìå Project Overview
Most ATS (Applicant Tracking Systems) reject qualified candidates because they lack specific keywords.

Unlike basic keyword matchers that fail on simple word variations (e.g., "Analyze" vs. "Analysis"), this **Phase 1** implementation integrates an **NLP Pipeline** to normalize text before matching. It identifies the gap between a candidate's resume and the job requirements using mathematical vectorization.

In **Phase 2**, we upgraded from simple keyword matching to **Semantic Understanding**. By integrating **BERT-based Transformers**, this tool now understands that *"Building Dashboards"* is semantically similar to *"Data Visualization"*, identifying qualified candidates even if they don't use the exact phrasing of the Job Description (JD).

The **Phase 3**, project is an **Engineering-Grade ATS Simulator** that evolves beyond simple scripts. It uses **Deep Learning (Transformers)** to understand the semantic meaning of a resume and is deployed as a **Dockerized Microservice**, ensuring it runs consistently on any machine (Local, AWS, Azure, etc.).

## üõ†Ô∏è Tech Stack
* **Deployment:** Docker & Docker Compose (Containerization).
* **Intelligence:** Sentence-Transformers (SBERT) & PyTorch (CPU-Optimized).
* **NLP Pipeline:** NLTK for text preprocessing and Lemmatization.
* **Frontend:** Streamlit.
* **Core Logic:** Hybrid Search (Cosine Similarity + Keyword Gap Analysis).

## ‚ú® Key Features
* **üê≥ Portable & Scalable:** Fully containerized. No "it works on my machine" issues‚Äîjust pull the image and run.
* **üß† Semantic Scoring:** Uses `all-MiniLM-L6-v2` to understand that "React" and "Frontend Development" are related.
* **‚ö° Instant Startup:** Uses **Docker Layer Caching** to pre-bake heavy AI models (80MB+) into the image, so the app launches instantly.
* **üîç Explainability:** Highlights the "Hero Sentences" that contributed most to your match score.

## üìÇ Project Structure
```text
resume-analyzer/
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ parser.py       # Handles PDF/DOCX extraction
‚îÇ   ‚îú‚îÄ‚îÄ cleaner.py      # NLTK Pipeline (Lemmatization & Cleaning)
‚îÇ   ‚îî‚îÄ‚îÄ analyzer.py     # Core logic (Cosine Similarity)
|‚îÄ‚îÄ Dockerfile          # Container instructions (Multi-layer build)
‚îú‚îÄ‚îÄ .dockerignore       # Build context optimization
‚îú‚îÄ‚îÄ app.py              # Main Streamlit Interface
‚îú‚îÄ‚îÄ requirements.txt    # Dependencies
‚îî‚îÄ‚îÄ README.md           # Documentation
```
üöÄ How to Run

--> Since this is Dockerized, you don't need to install Python or libraries manually.

Option 1: Run with Docker (Recommended)
1. Build the Image
```
docker build -t resume-analyzer .
```
Note: The build process automatically downloads the AI models, so they are "baked in" to the container.

2. Run the Container
```
docker run -p 8501:8501 resume-analyzer
```
3. Access App Open your browser to ``` http://localhost:8501 ```

üöÄ Option 2: Run Locally
1. Clone the Repository
```bash
git clone https://github.com/kanishk2705/Smart-Resume-Analyzer.git
cd resume-analyzer
```
2. Install Dependencies
```bash
pip install -r requirements.txt
```
3. Run the Application
```bash
streamlit run app.py
```
4. Usage
```bash
    . Upload your Resume (PDF or DOCX).
    . Paste the Job Description.
    . Click "Analyze" to see your score and missing keywords.
```
üó∫Ô∏è Engineering Roadmap

This project follows an iterative engineering path, moving from basic scripts to a production-grade AI application.

[x] Phase 1: NLP-Enhanced Matcher - Implemented NLTK pipeline for robust keyword matching.

[x] Phase 2: The Intelligence Layer - Integrating BERT/SBERT Transformers for Semantic Context.

[x] Phase 3: DevOps & Scalability - Dockerizing the application for portable deployment.

[ ] Phase 4: GenAI Mentor - Using LLMs (Gemini) to generate custom learning paths.

Built by A C KANISHK as part of an end-to-end Machine Learning Engineering study.